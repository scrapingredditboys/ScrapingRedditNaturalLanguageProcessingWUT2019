# -*- coding: utf-8 -*-
"""Sentiment compared to karma, timing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dTOJl3ooLBk6fiyiNHaWrj8pXl9ZAaNx

Function for loading files from Google Drive (needed, as that's where data is hosted):
"""

import requests

def download_file_from_google_drive(id, destination):
    def get_confirm_token(response):
        for key, value in response.cookies.items():
            if key.startswith('download_warning'):
                return value

        return None

    def save_response_content(response, destination):
        CHUNK_SIZE = 32768

        with open(destination, "wb") as f:
            for chunk in response.iter_content(CHUNK_SIZE):
                if chunk: # filter out keep-alive new chunks
                    f.write(chunk)

    URL = "https://docs.google.com/uc?export=download"

    session = requests.Session()

    response = session.get(URL, params = { 'id' : id }, stream = True)
    token = get_confirm_token(response)

    if token:
        params = { 'id' : id, 'confirm' : token }
        response = session.get(URL, params = params, stream = True)

    save_response_content(response, destination)

"""Load data:"""

download_file_from_google_drive("1ZAQOsTjJb0gss2_BffPquZpOAUmLVZKa", "subreddits.csv")
download_file_from_google_drive("12CN8BFShtAXWd-u8NHDMgwWiCH-6JKMG", "comments.csv")
download_file_from_google_drive("1Ag71q4kBsTAwWYomB_87wZvPU7n6acLp", "submissions.csv")
download_file_from_google_drive("1xpqjHUv-UBS5bUb0MWL3icd4G4Ce6uGZ", "comments_marked_0.csv")
download_file_from_google_drive("1Nib2NKaQ8JX75epsCbKPzRRE8uEsFZRy", "comments_marked_1.csv")
download_file_from_google_drive("1rXiH5268VSt4wmWdO6pJRaVnrX0tqcRj", "submissions_marked.csv")

"""Create data frames from files:"""

import pandas as pd

subreddits = pd.read_csv("subreddits.csv", sep='\t')
comments = pd.read_csv("comments.csv", sep='\t', parse_dates=['created_utc'])
comments.set_index('id', inplace=True)
submissions = pd.read_csv("submissions.csv", sep=',', parse_dates=['created_utc'])
submissions.set_index('id', inplace=True)
comments_marked_0 = pd.read_csv("comments_marked_0.csv", sep=',')
comments_marked_1 = pd.read_csv("comments_marked_1.csv", sep=',')
comments_marked = pd.concat([comments_marked_0, comments_marked_1])
comments_marked.set_index('comment', inplace=True)
submissions_marked = pd.read_csv("submissions_marked.csv", sep=',')
submissions_marked.set_index('submission', inplace=True)

comments_plus = pd.concat([comments, comments_marked], axis=1, join='inner')
submissions_plus = pd.concat([submissions, submissions_marked], axis=1, join='inner')

print(len(comments))
print(len(comments_plus))
print(len(submissions))
print(len(submissions_plus))

"""First topic: sentiment vs karma - how sentiment impacts score? Results as data points (for a single comment/submission)"""

import os.path
def sentiment_karma(name):
    sub_counter = 0
    comm_counter = 0
    # If file exists, then why calculate it again?
    if os.path.exists("sentiment_karma/sentiment_karma_comment_{0}.csv".format(name)):
        print("DONE\n")
        return
    df = pd.DataFrame(columns=["sentiment_pos_title", "sentiment_neg_title", "sentiment_pos_selftext", "sentiment_neg_selftext", "score", "upvote_ratio"])
    df2 = pd.DataFrame(columns=["sentiment_pos","sentiment_neg", "score"])

    subs = submissions_plus[submissions_plus['subreddit_id']==name]
    subs_count = len(subs)
    for submission_index, submission_row in subs.iterrows():
        sub_counter += 1
        print("Submission {0}/{1}".format(sub_counter, subs_count))
        df.loc[len(df)] = [submission_row["sentiment_pos_title"], 
                           submission_row["sentiment_neg_title"], 
                           submission_row["sentiment_pos_selftext"], 
                           submission_row["sentiment_neg_selftext"], 
                           submission_row['score'], submission_row['upvote_ratio']]

        # Comments
        comms = comments_plus[comments_plus['submission_id']==submission_index]
        for comment_index, comment_row in comms.iterrows():
            comm_counter += 1
            df2.loc[len(df2)] = [comment_row["sentiment_pos"], comment_row['sentiment_neg'], comment_row["score"]]

    df.sort_values(["score"], ascending=False)
    df2.sort_values(["score"], ascending=False)
    df.to_csv(path_or_buf="sentiment_karma/sentiment_karma_submissions_{0}.csv".format(name))
    df2.to_csv(path_or_buf="sentiment_karma/sentiment_karma_comments_{0}.csv".format(name))

print(submissions_marked[submissions_marked['submission']=='7wnzpo'])

import os
os.makedirs("sentiment_karma")
os.makedirs("sentiment_timing")

import threading
from multiprocessing.dummy import Pool as ThreadPool
pool = ThreadPool(8)
pool.map(sentiment_karma, subreddits['id'])

import shutil
shutil.make_archive("sentiment_karma.zip", 'zip', 'sentiment_karma')

"""Second measure: overall sentiment of posts and comments at a given time."""

def sentiment_time(name):
    sub_counter = 0
    df = pd.DataFrame(columns=["weekday", "hour", "submissions", "sentiment_pos_title", "sentiment_neg_title", "sentiment_pos_selftext", "sentiment_neg_selftext"])
    df2 = pd.DataFrame(columns=["weekday", "hour", "comments", "sentiment_pos", "sentiment_neg"])
    df["hour"] = list(range(0, 24)) * 7
    df2["hour"] = list(range(0, 24)) * 7
    for i in range(0, 7):
        df["weekday"][i * 24:(i + 1) * 24] = [i] * 24
        df2["weekday"][i * 24:(i + 1) * 24] = [i] * 24
    df.fillna(value=0.0, inplace=True)
    df2.fillna(value=0.0, inplace=True)
    df.set_index(['weekday', 'hour'], inplace=True, drop=True)
    df2.set_index(['weekday', 'hour'], inplace=True, drop=True)

    subs = submissions_plus[submissions_plus['subreddit_id']==name]
    subs_count = len(subs)
    for submission_index, submission_row in subs.iterrows():
        sub_counter += 1
        print("Submission {0}/{1}".format(sub_counter, subs_count))
        hour = submission_row['created_utc'].hour
        weekday = submission_row['created_utc'].weekday()
        karma = submission_row['score']
        df['submissions'][weekday, hour] += 1
        df['sentiment_pos_title'][weekday, hour] += submission_row['sentiment_pos_title']
        df['sentiment_neg_title'][weekday, hour] += submission_row['sentiment_neg_title']
        df['sentiment_pos_selftext'][weekday, hour] += submission_row['sentiment_pos_selftext']
        df['sentiment_neg_selftext'][weekday, hour] += submission_row['sentiment_neg_selftext']

        comms = comments_plus[comments_plus['submission_id']==submission_index]
        for _, comment_row in comms.iterrows():
            hour = comment_row['created_utc'].hour
            weekday = comment_row['created_utc'].weekday()
            df2['comments'][weekday, hour] += 1
            df2['sentiment_pos'][weekday, hour] += comment_row['sentiment_pos']
            df2['sentiment_neg'][weekday, hour] += comment_row['sentiment_neg']

    df.to_csv(path_or_buf="sentiment_timing/sentiment_submission_timing_{0}.csv".format(name))
    df2.to_csv(path_or_buf="sentiment_timing/sentiment_comment_timing_{0}.csv".format(name))

import threading
from multiprocessing.dummy import Pool as ThreadPool
pool = ThreadPool(8)
pool.map(sentiment_time, subreddits['id'])

import shutil
shutil.make_archive("sentiment_timing", 'zip', 'sentiment_timing')

